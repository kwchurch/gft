name: roberta_model_0	labels (NA): NA	RobertaModel(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(50265, 384, padding_idx=1, sparse=False)
    (position_embeddings): Embedding(514, 384, sparse=False)
    (token_type_embeddings): Embedding(1, 384, sparse=False)
    (layer_norm): LayerNorm(normalized_shape=[384], epsilon=1e-05)
    (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
  )
  (encoder): TransformerEncoder(
    (layers): LayerList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (k_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (v_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (out_proj): Linear(in_features=384, out_features=384, dtype=float32)
        )
        (linear1): Linear(in_features=384, out_features=1536, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=1536, out_features=384, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (k_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (v_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (out_proj): Linear(in_features=384, out_features=384, dtype=float32)
        )
        (linear1): Linear(in_features=384, out_features=1536, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=1536, out_features=384, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (k_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (v_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (out_proj): Linear(in_features=384, out_features=384, dtype=float32)
        )
        (linear1): Linear(in_features=384, out_features=1536, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=1536, out_features=384, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (k_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (v_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (out_proj): Linear(in_features=384, out_features=384, dtype=float32)
        )
        (linear1): Linear(in_features=384, out_features=1536, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=1536, out_features=384, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (k_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (v_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (out_proj): Linear(in_features=384, out_features=384, dtype=float32)
        )
        (linear1): Linear(in_features=384, out_features=1536, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=1536, out_features=384, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (k_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (v_proj): Linear(in_features=384, out_features=384, dtype=float32)
          (out_proj): Linear(in_features=384, out_features=384, dtype=float32)
        )
        (linear1): Linear(in_features=384, out_features=1536, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=1536, out_features=384, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[384], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=384, out_features=384, dtype=float32)
    (activation): Tanh()
  )
)
