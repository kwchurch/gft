name: gpt_model_0	labels (NA): NA	GPTModel(
  (embeddings): GPTEmbeddings(
    (word_embeddings): Embedding(50257, 1600, sparse=False)
    (position_embeddings): Embedding(1024, 1600, sparse=False)
    (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
  )
  (decoder): TransformerDecoder(
    (layers): LayerList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (6): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (7): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (8): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (9): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (10): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (11): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (12): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (13): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (14): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (15): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (16): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (17): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (18): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (19): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (20): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (21): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (22): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (23): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (24): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (25): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (26): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (27): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (28): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (29): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (30): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (31): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (32): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (33): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (34): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (35): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (36): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (37): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (38): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (39): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (40): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (41): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (42): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (43): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (44): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (45): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (46): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (47): TransformerDecoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (k_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (v_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
          (out_proj): Linear(in_features=1600, out_features=1600, dtype=float32)
        )
        (linear1): Linear(in_features=1600, out_features=6400, dtype=float32)
        (linear2): Linear(in_features=6400, out_features=1600, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (norm2): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
    )
    (norm): LayerNorm(normalized_shape=[1600], epsilon=1e-05)
  )
)
