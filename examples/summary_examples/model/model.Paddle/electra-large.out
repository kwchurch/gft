name: electra_model_0	labels (NA): NA	ElectraModel(
  (embeddings): ElectraEmbeddings(
    (word_embeddings): Embedding(30522, 1024, sparse=False)
    (position_embeddings): Embedding(512, 1024, sparse=False)
    (token_type_embeddings): Embedding(2, 1024, sparse=False)
    (layer_norm): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
    (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
  )
  (encoder): TransformerEncoderPro(
    (layers): LayerList(
      (0): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (1): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (2): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (3): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (4): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (5): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (6): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (7): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (8): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (9): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (10): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (11): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (12): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (13): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (14): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (15): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (16): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (17): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (18): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (19): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (20): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (21): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (22): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
      (23): TransformerEncoderLayerPro(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
        )
        (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
        (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
        (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)
        (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
      )
    )
  )
)
