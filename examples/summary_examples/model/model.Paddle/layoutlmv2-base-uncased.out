name: layout_l_mv2_model_0	labels (NA): NA	LayoutLMv2Model(
  (embeddings): LayoutLMv2Embeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0, sparse=False)
    (position_embeddings): Embedding(512, 768, sparse=False)
    (x_position_embeddings): Embedding(1024, 128, sparse=False)
    (y_position_embeddings): Embedding(1024, 128, sparse=False)
    (h_position_embeddings): Embedding(1024, 128, sparse=False)
    (w_position_embeddings): Embedding(1024, 128, sparse=False)
    (token_type_embeddings): Embedding(2, 768, sparse=False)
    (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
    (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
  )
  (visual): VisualBackbone(
    (backbone): FPN(
      (fpn_lateral2): Conv2d(256, 256, kernel_size=[1, 1], data_format=NCHW)
      (fpn_output2): Conv2d(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (fpn_lateral3): Conv2d(512, 256, kernel_size=[1, 1], data_format=NCHW)
      (fpn_output3): Conv2d(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (fpn_lateral4): Conv2d(1024, 256, kernel_size=[1, 1], data_format=NCHW)
      (fpn_output4): Conv2d(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (fpn_lateral5): Conv2d(2048, 256, kernel_size=[1, 1], data_format=NCHW)
      (fpn_output5): Conv2d(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (top_block): LastLevelMaxPool()
      (bottom_up): ResNet(
        (stem): BasicStem(
          (conv1): Conv2d(3, 64, kernel_size=[7, 7], stride=[2, 2], padding=3, data_format=NCHW
            (norm): FrozenBatchNorm()
          )
        )
        (res2): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(64, 256, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv1): Conv2d(64, 256, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(256, 256, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(256, 256, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(256, 256, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(256, 256, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(256, 256, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(256, 256, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(256, 256, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(256, 256, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
        )
        (res3): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(256, 512, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv1): Conv2d(256, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(512, 512, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(512, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(512, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(512, 512, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(512, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(512, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(512, 512, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(512, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(512, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(512, 512, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(512, 512, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
        )
        (res4): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(512, 1024, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv1): Conv2d(512, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (3): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (4): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (5): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (6): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (7): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (8): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (9): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (10): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (11): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (12): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (13): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (14): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (15): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (16): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (17): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (18): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (19): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (20): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (21): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (22): BottleneckBlock(
            (conv1): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(1024, 1024, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(1024, 1024, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
        )
        (res5): Sequential(
          (0): BottleneckBlock(
            (shortcut): Conv2d(1024, 2048, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv1): Conv2d(1024, 2048, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(2048, 2048, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(2048, 2048, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (1): BottleneckBlock(
            (conv1): Conv2d(2048, 2048, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(2048, 2048, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(2048, 2048, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
          (2): BottleneckBlock(
            (conv1): Conv2d(2048, 2048, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv2): Conv2d(2048, 2048, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW
              (norm): FrozenBatchNorm()
            )
            (conv3): Conv2d(2048, 2048, kernel_size=[1, 1], data_format=NCHW
              (norm): FrozenBatchNorm()
            )
          )
        )
      )
    )
    (pool): AdaptiveAvgPool2D(output_size=[7, 7])
  )
  (visual_proj): Linear(in_features=256, out_features=768, dtype=float32)
  (visual_LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
  (visual_dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
  (encoder): LayoutLMv2Encoder(
    (layer): LayerList(
      (0): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (1): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (2): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (3): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (4): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (5): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (6): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (7): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (8): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (9): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (10): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (11): LayoutLMv2Layer(
        (attention): LayoutLMv2Attention(
          (self): LayoutLMv2SelfAttention(
            (qkv_linear): Linear(in_features=768, out_features=2304, dtype=float32)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
          (output): LayoutLMv2SelfOutput(
            (dense): Linear(in_features=768, out_features=768, dtype=float32)
            (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
            (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          )
        )
        (intermediate): LayoutLMv2Intermediate(
          (dense): Linear(in_features=768, out_features=3072, dtype=float32)
          (intermediate_act_fn): GELU(approximate=False)
        )
        (output): LayoutLMv2Output(
          (dense): Linear(in_features=3072, out_features=768, dtype=float32)
          (LayerNorm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
          (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
    )
    (rel_pos_bias): Linear(in_features=32, out_features=12, dtype=float32)
    (rel_pos_x_bias): Linear(in_features=64, out_features=12, dtype=float32)
    (rel_pos_y_bias): Linear(in_features=64, out_features=12, dtype=float32)
  )
  (pooler): LayoutLMv2Pooler(
    (dense): Linear(in_features=768, out_features=768, dtype=float32)
    (activation): Tanh()
  )
)
