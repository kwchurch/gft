name: m_bart_model_0	labels (NA): NA	MBartModel(
  (shared): Embedding(250027, 1024, padding_idx=1, sparse=False)
  (encoder): MBartEncoder(
    (embed_tokens): Embedding(250027, 1024, padding_idx=1, sparse=False)
    (encoder_embed_positions): MBartLearnedPositionalEmbedding(1026, 1024, padding_idx=1, sparse=False)
    (encoder_dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
    (encoder_layernorm_embedding): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
    (encoder): TransformerEncoder(
      (layers): LayerList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (norm): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
    )
  )
  (decoder): MBartDecoder(
    (embed_tokens): Embedding(250027, 1024, padding_idx=1, sparse=False)
    (decoder_embed_positions): MBartLearnedPositionalEmbedding(1026, 1024, padding_idx=1, sparse=False)
    (decoder_dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
    (decoder_layernorm_embedding): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
    (decoder): TransformerDecoder(
      (layers): LayerList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (cross_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
            (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)
          )
          (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)
          (dropout): Dropout(p=0.0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (norm3): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout3): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
      (norm): LayerNorm(normalized_shape=[1024], epsilon=1e-05)
    )
  )
)
